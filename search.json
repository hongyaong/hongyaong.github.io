[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Welcome to Hong Yao’s project.\nClick on DSA306 Customer Churn Prediction Analysis to look at my school project in SMU."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About our dataset\n\n\n\n\n\nVariable\nMeaning\nDataType\nNotes\n\n\n\n\nCustomerID\nUnique Identifier that identifies each customer\nint\nNA\n\n\nAge\nAge of the customer in years\nint\nNA\n\n\nGender\nGender of the customer\nchr\nMale or Female\n\n\nTenure\nTotal amount of of months the customer has been with the company\nint\nNA\n\n\nUsage Frequency\nHow often the customer uses the service\nint\nNA\n\n\nSupport Calls\nNumber of customer support calls made by the customer\nint\nNA\n\n\nPayment Delay\nNumber of days a customer's payment is delayed.\nint\nNA\n\n\nSubscription Type\nType of subscription the customer subscribed for\nchr\nStandard, Basic or Premium\n\n\nContract Length\nLength of customer's contract\nchr\nAnnual, Monthly or Quarterly\n\n\nTotal Spend\nTotal amount spent by the customer on the service in USD\nnum\nNA\n\n\nLast Interaction\nThe number of days since the customer's last interaction with the company\nint\nNA\n\n\nChurn\nWhether the customer has churned or not\nint\nChurned = 1, Not Churned = 0"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Attaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'sparklyr'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\n\n\n# A tibble: 1 × 12\n  CustomerID   Age Gender Tenure `Usage Frequency` `Support Calls`\n       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n1          1     1      0      1                 1               1\n# ℹ 6 more variables: `Payment Delay` &lt;dbl&gt;, `Subscription Type` &lt;dbl&gt;,\n#   `Contract Length` &lt;dbl&gt;, `Total Spend` &lt;dbl&gt;, `Last Interaction` &lt;dbl&gt;,\n#   Churn &lt;dbl&gt;\n\n\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   29.00   39.00   39.37   48.00   65.00 \n\n\nLooking at the distribution by age, a large portion of our dataset comes from aged 40-50 years old and a low portion comes from age 50-60 years old.\nThe statistic summary of age shows that the mean age is 39.37 and it’s ranging from 18 to 65 years old.\n\n\n\n\n\nFrom the Gender Distribution bar chart, we are able to see that we have more Male than Female in this dataset.\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   16.00   32.00   31.26   46.00   60.00 \n\n\nLooking at the Distribution by Tenure, it is apparent that the dataset exhibits a generally balanced distribution where majority of Tenure values hover around range of 7000 - 8000 in terms of count. However, for tenure values falling within the ranges of 10-20 and 0-5, there is a slight drop with counts ranging from 6000 to 7000, indicating a comparatively lower frequency of occurrences within these specific tenure ranges.”\nThe statistic summary of tenure shows that the dataset mean tenure is 31.26 and it’s ranging from 1 to 60 Tenure.\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    9.00   16.00   15.81   23.00   30.00 \n\n\nLooking at the Distribution by Usage Frequency, it is obvious that the dataset exhibits a generally balanced distribution where majority of Tenure values hover around range of 15000 in terms of count. However, for Usage Frequency values &lt; 10, the counts is slightly lower.\nThe statistic summary of Usage Frequency shows that the dataset mean Usage Frequency is 15.81 and it’s ranging from 1 to 30 Usage Frequency.\n\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   3.000   3.604   6.000  10.000 \n\n\nLooking at the Distribution by Support Calls, there is a downward trend in terms of counts as support calls increases. Support calls of 5 onward has a stable frequency of around 20000.\nThe statistic summary of Support Calls shows that the dataset mean Support Calls is 3.604 and it’s ranging from 0 to 10 calls."
  },
  {
    "objectID": "abouttest.html",
    "href": "abouttest.html",
    "title": "abouttest",
    "section": "",
    "text": "Variable\nMeaning\nDataType\nNotes\n\n\n\n\nCustomerID\nUnique Identifier that identifies each customer\nint\nNA\n\n\nAge\nAge of the customer in years\nint\nNA\n\n\nGender\nGender of the customer\nchr\nMale or Female\n\n\nTenure\nTotal amount of of months the customer has been with the company\nint\nNA\n\n\nUsage Frequency\nHow often the customer uses the service\nint\nNA\n\n\nSupport Calls\nNumber of customer support calls made by the customer\nint\nNA\n\n\nPayment Delay\nNumber of days a customer's payment is delayed.\nint\nNA\n\n\nSubscription Type\nType of subscription the customer subscribed for\nchr\nStandard, Basic or Premium\n\n\nContract Length\nLength of customer's contract\nchr\nAnnual, Monthly or Quarterly\n\n\nTotal Spend\nTotal amount spent by the customer on the service in USD\nnum\nNA\n\n\nLast Interaction\nThe number of days since the customer's last interaction with the company\nint\nNA\n\n\nChurn\nWhether the customer has churned or not\nint\nChurned = 1, Not Churned = 0"
  },
  {
    "objectID": "about_dataset.html",
    "href": "about_dataset.html",
    "title": "About Dataset",
    "section": "",
    "text": "Variable\nMeaning\nDataType\nNotes\n\n\n\n\nCustomerID\nUnique Identifier that identifies each customer\nint\nNA\n\n\nAge\nAge of the customer in years\nint\nNA\n\n\nGender\nGender of the customer\nchr\nMale or Female\n\n\nTenure\nTotal amount of of months the customer has been with the company\nint\nNA\n\n\nUsage Frequency\nHow often the customer uses the service\nint\nNA\n\n\nSupport Calls\nNumber of customer support calls made by the customer\nint\nNA\n\n\nPayment Delay\nNumber of days a customer's payment is delayed.\nint\nNA\n\n\nSubscription Type\nType of subscription the customer subscribed for\nchr\nStandard, Basic or Premium\n\n\nContract Length\nLength of customer's contract\nchr\nAnnual, Monthly or Quarterly\n\n\nTotal Spend\nTotal amount spent by the customer on the service in USD\nnum\nNA\n\n\nLast Interaction\nThe number of days since the customer's last interaction with the company\nint\nNA\n\n\nChurn\nWhether the customer has churned or not\nint\nChurned = 1, Not Churned = 0"
  },
  {
    "objectID": "DSA306project.html",
    "href": "DSA306project.html",
    "title": "DSA306 Customer Churn Prediction Analysis",
    "section": "",
    "text": "Introduction\nFor this project, our group worked on a customer churn dataset. This dataset is centred around a telecommunications company, where it provides information on how customers are utilising the services and also indicates whether a customer has churned. Customer churn denotes whether a customer has decided to discontinue their subscription to the company’s mobile services within a period of time. It allows the company to assess factors contributing to customer turnover and make informed decisions to retain customers. This project aims to propose a model to predict potential customer churners. To do so, spark will be leveraged to push computations and techniques such as exploratory data analysis and modelling will be employed.\nDataset codebook\n\n\n\n\n\nVariable\nMeaning\nDataType\nNotes\n\n\n\n\nCustomerID\nUnique Customer Identifier\nString\nNA\n\n\nAge\nCustomer's Age in Years\nInteger\n18 to 65\n\n\nGender\nCustomer's Gender\nString\nMale or Female\n\n\nTenure\nLength of Customer Subscription in Months\nInteger\n1 to 60\n\n\nUsage Frequency\nCustomer’s Frequency of Daily Service Usage\nInteger\n1 to 30\n\n\nSupport Calls\nNumber of Support Calls made by Customer\nInteger\n0 (None) to 10\n\n\nPayment Delay\nCustomer’s Payment Delay in Days\nInteger\n0 (None) to 30\n\n\nSubscription Type\nChosen Subscription by Customer\nString\nBasic, Standard or Premium\n\n\nContract Length\nCustomer Service Contract Duration\nString\nMonthly, Quarterly or Annually\n\n\nTotal Spend\nCustomer Total Spending on Service in USD\nFloat\nNA\n\n\nLast Interaction\nDays since Customer Last Interact with the Company\nInteger\n1 to 30\n\n\nChurn\nCustomer Discontinuation with the Service\nInteger\n1 (Yes) or 0 (No)\n\n\n\n\n\n\n\nExploratory Data Analysis (EDA)\nTo better understand our dataset, we conducted EDA to examine the distribution of independent variables and their relationships with the dependent variable. To enhance the clarity in our visualizations, we labelled Churn (1 and 0) as “Churn” and “Non-Churn” respectively. The figures accompanying our analysis can be found in the Appendix.\n\n\nFileSystemDataset (query)\nCustomerID: string\nAge: int64\nGender: string\nTenure: int64\nUsage Frequency: int64\nSupport Calls: int64\nPayment Delay: int64\nSubscription Type: string\nContract Length: string\nTotal Spend: float\nLast Interaction: int64\nChurn: int64\n\nSee $.data for the source Arrow object\n\n\n\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n\n\n\nWe observe an equal distribution of churn and non-churn customers, with 280,492 and 224,714 instances, respectively. This balanced class distribution ensures that the dataset will not introduce bias in model prediction and prevent the model from favouring the majority class.\n\n\n\n\n\nWe can observe that among churn customers, age is uniformly distributed across the 20 to 60 years old range. In contrast, non-churn customers tend to be concentrated between 30 and 50 years old, with a notable decrease in the proportion of aged 50 to 60 years old. Since there is a notable difference in the distribution, Age is possibly a significant variable.\n\n\n\n\n\nWe notice that among churn customers, the number of males and females is quite similar. In contrast, among non-churn customers, there is a considerably higher count of males compared to females. Disparity in distribution suggests that Gender might be significant.\n\n\n\n\n\nThe distribution of tenure does not show a notable disparity between churn and non-churn customers. There are minor variations, with non-churn customers showing slightly higher and lower proportions in the 10 and 20 months range respectively. This suggests that Tenure might be insignificant.\n\n\n\n\n\nThere is a small difference in the distribution of usage frequency within the 0-10 days range, with a slightly higher proportion of churn customers compared to non-churn customers. As such, Usage Frequency might be insignificant.\n\n\n\n\n\nThe majority of non-churn customers make fewer than 4 support calls. As a result, for support call counts less than 4, there are more non-churn customers than churn customers, while the opposite is observed for support call counts between 4 and 10. Notable difference in distribution suggests that Support Calls are significant.\n\n\n\n\n\nWe observe that non-churn customers are concentrated in less than 20 days. However, among churn customers, the distribution is more even, with a slightly higher proportion having payment delays exceeding 20 days. Disparity in distribution seems to suggest that Payment Delay is significant.\n\n\n\n\n\nWe do not observe any distinction in the subscription preferences among churn and non-churn customers respectively. Therefore, Subscription Type might be insignificant.\n\n\n\n\n\nWe noticed that non-churn customers tend to avoid signing up for monthly contracts. Whereas, for churn customers, there is a relatively even distribution. Huge difference in distribution suggests that Contact Length might be significant.\n\n\n\n\n\nWe can observe that churn customers exhibit a relatively even distribution in terms of total amount spent, whereas non-churn customers tend to concentrate on total amount spent exceeding $500 range. Disparity in distribution seems to suggest that Total Spend might be significant.\n\n\n\n\n\nWe observe that churn customers show a relatively even distribution in terms of the number of days since their last interaction with the company. In contrast, a significant proportion of non-churn customers are concentrated around less than 15 days since their last interaction. Notable difference in distribution suggests that Last Interaction is significant.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(columns)\n\n  # Now:\n  data %&gt;% select(all_of(columns))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nNew names:\nCorrelation method: 'pearson' Missing treated using: 'pairwise.complete.obs'\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n\n\n\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\nWe observe the absence of significant relationships among the predictors, which indicates a low probability of multicollinearity. Hence, these attributes enhance the model capacity to independently estimate the relationship between independent and target variables. A significant correlation coefficient of 0.52 is observed between Churn and Support Calls. This implies that a higher number of support calls may reflect underlying product issues, leading to increased customer difficulties and a greater likelihood of subscription cancellations, thus, increasing churn rates\nFeature Engineering\nWe have applied various feature engineering techniques to pre-process our features and enhance their quality, before training our models to improve the overall performance.\nWe performed data binning on selected numeric variables, transforming them into categorical variables (Figure 14). Based on our EDA findings, we identify three variables - “Age”, “Total Spend” and “Payment Delay” - that display a non-monotonic relationship (step function) with the target variable. These variables exhibit a uniform distribution within specific intervals, leading us to bin them accordingly based on those intervals.\nWe built a Spark ML pipeline to integrate a series of transformation steps and algorithms on our train dataset into a single workflow. In the initial stage, we utilize the ft_standard_scaler function to standardise our numeric variables on a constant scale. This ensures that our numeric variables maintain a uniform scale and thus, prevents the model from being biassed towards variables with larger scales. Subsequently, we utilise ft_one_hot_encoder functions to encode categorical variables into numerical representations, generating binary columns for each distinct category (except for the base). This allows us to include those variables into our models since most algorithms require numeric inputs. In the final step of our process, following the transformation of our variables, we apply the ml_logistic_regression function on our dataset, given its suitability in binary classification and its ease in interpreting results.\nModel Training and Evaluation We utilise the sdf_random_split function to partition our dataset into training and test subsets, with a 70-30 ratio, and set the seed to 1234 to ensure reproducibility. After constructing our ML pipeline, we employ the ml_fit function to fit the training dataset into the pipeline, generating a pipeline model. We then utilize the ml_transform function to execute the pipeline model on the test dataset which allows us to assess the performance of our model and evaluate its effectiveness in handling unseen data.\nWe decided to utilise three metrics - AreaUnderROC, Precision and Recall - to evaluate the performance of our model using the test dataset. They range from 0 to 1, with a higher value indicating better performance. The AreaUnderROC is useful in evaluating the model’s ability to classify between churn and non-churn customers at various thresholds.\nA confusion matrix was created to calculate the Precision and Recall of our model performance. Precision evaluates the accuracy of positive predictions made by our model, providing us with the proportion of actually churned customers among those predicted as churned. Recall measures our model’s ability to capture all actual churned customers, providing the proportion of correctly predicted churn customers among those who actually churned.\nHaving obtained our models, we can then look to employ regularisation techniques. Regularisation will help prevent overfitting of our model by imposing penalties on our coefficients. There are 3 regularisation techniques considered - LASSO Regularisation, Ridge Regularisation and Elastic Net Regularisation. In order to employ these 3 techniques, we have utilised the ml_cross_validator function to specify our alpha and lambda values. Whereby, elastic_net_param (alpha) has been set from 0 to 1 in increments of 0.2 and reg_param (lambda) has taken the values of 0.001, 0.005 and 0.01. Performance Metric ROC is then used to identify the best set of hyperparameters to be reimplemented into the initial pipeline.\nLogistic Modelling - Model 1\nWe initially begin with a base model, utilising all the available variables in the dataset and maintaining the default threshold within the model function.\n\n\n[1] \"AUC for model 1: 0.935469957012298\"\n\n\n[1] \"Confusion matrix for model 1:\"\n\n\n     [,1]  [,2] \n[1,] 73745 10198\n[2,] 8608  58403\n\n\n  accuracy precision Type 1 error Type 2 error  Recall_1\n1 0.875419 0.8954744    0.1284565    0.1214872 0.8785128\n\n\nLogistic Modelling - Model 1 (CV)\nApplying regularisation, it was determined that the best set of values were 0 and 0.001 for alpha and lambda values respectively. This indicates that a LASSO regularisation was favoured over the other 2 regularisation techniques. Model 1 (CV)’s performance metrics can be viewed in the table below.\n\n\n   areaUnderROC reg_param_1 elastic_net_param_1\n1     0.9353870       0.001                 0.8\n2     0.9353867       0.001                 1.0\n3     0.9353857       0.001                 0.6\n4     0.9353814       0.001                 0.4\n5     0.9353745       0.001                 0.2\n6     0.9353680       0.001                 0.0\n7     0.9352370       0.005                 0.0\n8     0.9352312       0.005                 0.2\n9     0.9352006       0.005                 0.4\n10    0.9351575       0.005                 0.6\n11    0.9350910       0.005                 0.8\n12    0.9350403       0.010                 0.0\n13    0.9349921       0.005                 1.0\n14    0.9349547       0.010                 0.2\n15    0.9348011       0.010                 0.4\n16    0.9345303       0.010                 0.6\n17    0.9341034       0.010                 0.8\n18    0.9334750       0.010                 1.0\n\n\n[1] \"AUC for model 1a: 0.935482660776576\"\n\n\n[1] \"Confusion matrix for model 1a:\"\n\n\n     [,1]  [,2] \n[1,] 73690 10253\n[2,] 8536  58475\n\n\n   accuracy precision Type 1 error Type 2 error    Recall\n1 0.8755316 0.8961886    0.1273821    0.1221424 0.8778576\n\n\nIt is observed that there are slight improvements to the Model 1 (CV)’s performance relative to Model 1. In which, we observe that the ROC and Precision values increase in its 4th and 3rd significant figure respectively. Whereas, a decrease in Recall in its 3rd significant figure is observed.\nLogistic Modelling - Model 2\nWe then look to create an alternative model that seeks to drop variables that seem to have less explanatory power in determining churned and non-churned customers. This is done primarily through EDA. It is subsequently determined that variables - Tenure, Subscription Type and Usage Frequency be dropped.\n\n\n[1] \"AUC for model 2: 0.935282038899156\"\n\n\n[1] \"Confusion matrix for model 2:\"\n\n\n     [,1]  [,2] \n[1,] 73731 10212\n[2,] 8610  58401\n\n\n  accuracy precision Type 1 error Type 2 error   Recall\n1 0.875313 0.8954348    0.1284864     0.121654 0.878346\n\n\nLogistic Modelling - Model 2 (CV)\nAfter regularisation, it was determined that the best set of values were 0 and 0.001 for alpha and lambda values respectively. This indicates that a LASSO regularisation was favoured over the other 2 regularisation techniques. Model 2 (CV)’s performance metrics can be viewed in the table below.\n\n\n   areaUnderROC elastic_net_param_1 reg_param_1\n1     0.9351241                 1.0       0.001\n2     0.9351225                 0.8       0.001\n3     0.9351180                 0.6       0.001\n4     0.9351135                 0.4       0.001\n5     0.9351065                 0.2       0.001\n6     0.9351049                 0.0       0.001\n7     0.9349758                 0.0       0.005\n8     0.9349700                 0.2       0.005\n9     0.9349453                 0.4       0.005\n10    0.9349046                 0.6       0.005\n11    0.9348419                 0.8       0.005\n12    0.9347790                 0.0       0.010\n13    0.9347500                 1.0       0.005\n14    0.9347042                 0.2       0.010\n15    0.9345486                 0.4       0.010\n16    0.9342925                 0.6       0.010\n17    0.9338973                 0.8       0.010\n18    0.9333120                 1.0       0.010\n\n\n[1] \"AUC for model 2a: 0.935280551815638\"\n\n\n[1] \"Confusion matrix for model 2a:\"\n\n\n     [,1]  [,2] \n[1,] 73696 10247\n[2,] 8568  58443\n\n\n   accuracy precision Type 1 error Type 2 error    Recall\n1 0.8753594 0.8958475    0.1278596    0.1220709 0.8779291\n\n\nComparing the differences in performance metrics between Model 3 and Model 3 (CV), there are little differences in their values. The same ROC values are obtained, Precision and Recall values increased and decreased in its 4th significant figure respectively.\nConclusion\nObserving across the performance metrics of the proposed models, there are little changes in values. In particular, changes including improvements to the values all occur within the 3rd or 4th significant figure which arguably does not represent major improvements across the models. Hence, this suggests that the models proposed have similar predictive capabilities. Keeping this in mind, we can pivot our focus to the principle of parsimony which favours simpler models over complex models without compromising on predictive power. Hence, the principle of parsimony dictates that Model 2 is more favourable due to having fewer variables. Furthermore, to prevent overfitting, regularisation techniques are applied, thus, we can conclude that Model 2 (CV) is an overall better model.\n\n\nModel successfully saved.\n\n\nCredits\nChao Soon En Joy Chen Xiangzhen Samson Rachel Tan Yan Ning Wee Su Ying"
  }
]